{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20507d1-3e07-4a85-9101-158448e94d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crocodile import Crocodile\n",
    "import os\n",
    "# Create an instance of the Crocodile class\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    table_trace_collection_name=\"table_trace\",\n",
    "    dataset_trace_collection_name=\"dataset_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"],  # Access the entity retrieval token directly from environment variables\n",
    "    max_workers=50,\n",
    "    candidate_retrieval_limit=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394a9e06-61c7-42fc-8ce7-3019fd381b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Q57710097',\n",
       "  'name': 'Alsaka',\n",
       "  'description': 'family name',\n",
       "  'types': [{'id': 'Q101352', 'name': 'family name'}],\n",
       "  'features': {'ntoken_mention': 1,\n",
       "   'ntoken_entity': 1,\n",
       "   'length_mention': 6,\n",
       "   'length_entity': 6,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 1.0,\n",
       "   'jaccard_score': 1.0,\n",
       "   'jaccardNgram_score': 1.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.0,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': None}},\n",
       " {'id': 'Q57710056',\n",
       "  'name': 'Yacoub Alsaka',\n",
       "  'description': 'Professor of Engineering, University of Central Florida',\n",
       "  'types': [{'id': 'Q5', 'name': 'human'},\n",
       "   {'id': 'Q1622272', 'name': 'university teacher'}],\n",
       "  'features': {'ntoken_mention': 1,\n",
       "   'ntoken_entity': 2,\n",
       "   'length_mention': 6,\n",
       "   'length_entity': 13,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 0.46,\n",
       "   'jaccard_score': 0.5,\n",
       "   'jaccardNgram_score': 0.5,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.0,\n",
       "   'kind': 1,\n",
       "   'NERtype': 3,\n",
       "   'column_NERtype': None}},\n",
       " {'id': 'Q61463370',\n",
       "  'name': 'Mineral resources of Alaska : report on progress of investigations in 1911 -- Placer mining and water supply of the Fairbanks, Circle, Fortymile, Eagle, and Seventymile River districts, Alsaka',\n",
       "  'description': 'scholarly article by Clarence E. Ellsworth & E.A. Porter published 1912 in U.S. Geological Survey Bulletin',\n",
       "  'types': [{'id': 'Q13442814', 'name': 'scholarly article'}],\n",
       "  'features': {'ntoken_mention': 1,\n",
       "   'ntoken_entity': 29,\n",
       "   'length_mention': 6,\n",
       "   'length_entity': 192,\n",
       "   'popularity': 0.0,\n",
       "   'ed_score': 0.03,\n",
       "   'jaccard_score': 0.04,\n",
       "   'jaccardNgram_score': 0.04,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.0,\n",
       "   'kind': 1,\n",
       "   'NERtype': 4,\n",
       "   'column_NERtype': None}},\n",
       " {'id': 'Q797',\n",
       "  'name': 'Alaska',\n",
       "  'description': 'state of the United States of America',\n",
       "  'types': [{'id': 'Q35657', 'name': 'U.S. state'},\n",
       "   {'id': 'Q933394', 'name': 'exclave'}],\n",
       "  'features': {'ntoken_mention': 1,\n",
       "   'ntoken_entity': 1,\n",
       "   'length_mention': 6,\n",
       "   'length_entity': 6,\n",
       "   'popularity': 0.29,\n",
       "   'ed_score': 0.67,\n",
       "   'jaccard_score': 0.0,\n",
       "   'jaccardNgram_score': 0.0,\n",
       "   'desc': 0.0,\n",
       "   'descNgram': 0.0,\n",
       "   'bow_similarity': 0.0,\n",
       "   'kind': 1,\n",
       "   'NERtype': 1,\n",
       "   'column_NERtype': None}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crocodile_instance.fetch_candidates(\"alsaka\", \"\", qid=\"Q797\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4b9884-17d6-4b8d-94d9-cdad02c8b9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped collection: input_data\n",
      "Dropped collection: training_data\n",
      "Dropped collection: dataset_trace\n",
      "Dropped collection: timing_trace\n",
      "Dropped collection: table_trace\n",
      "Dropped collection: process_queue\n",
      "Dropped collection: web_requests\n",
      "All unwanted collections have been dropped.\n",
      "Data onboarded successfully for dataset 'test' and table 'imdb_top_1000_speed_test'.\n",
      "Found 1000 tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "All tasks have been processed.\n",
      "Entity linking process completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from crocodile import Crocodile\n",
    "import os\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './tables/imdb_top_1000.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "# Drop the entire crocodile_db database\n",
    "#client.drop_database(\"crocodile_db\")\n",
    "db = client[\"crocodile_db\"]\n",
    "\n",
    "# Drop all collections except 'bow_cache' and 'candidate_cache'\n",
    "collections_to_keep = [\"bow_cache\", \"candidate_cache\"]\n",
    "all_collections = db.list_collection_names()\n",
    "\n",
    "for collection in all_collections:\n",
    "    if collection not in collections_to_keep:\n",
    "        db[collection].drop()\n",
    "        print(f\"Dropped collection: {collection}\")\n",
    "\n",
    "print(\"All unwanted collections have been dropped.\")\n",
    "\n",
    "\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "process_queue = db[\"process_queue\"]\n",
    "\n",
    "dataset_name = \"test\"\n",
    "table_name = \"imdb_top_1000_speed_test\"\n",
    "\n",
    "\n",
    "# Ensure indexes for uniqueness and performance\n",
    "def ensure_indexes():\n",
    "    input_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING), (\"row_id\", ASCENDING)], unique=True)\n",
    "    table_trace_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    dataset_trace_collection.create_index([(\"dataset_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"status\", ASCENDING)])  # Ensure fast retrieval of items by status\n",
    "\n",
    "ensure_indexes()\n",
    "\n",
    "# Define column classifications for NE and LIT types\n",
    "ne_cols = {\n",
    "    \"0\": \"OTHER\",    # Series_Title\n",
    "    \"7\": \"PERSON\",   # Director\n",
    "    \"8\": \"PERSON\"    # Star1\n",
    "}\n",
    "\n",
    "lit_cols = {\n",
    "    \"1\": \"NUMBER\",   # Released_Year\n",
    "    \"2\": \"NUMBER\",   # Runtime (min)\n",
    "    \"3\": \"STRING\",    # Genre\n",
    "    \"4\": \"NUMBER\",   # IMDB_Rating\n",
    "    \"5\": \"STRING\",   # Overview\n",
    "    \"6\": \"NUMBER\",   # Meta_score\n",
    "    \"9\": \"NUMBER\",   # No_of_Votes\n",
    "    \"10\": \"NUMBER\"   # Gross\n",
    "}\n",
    "\n",
    "# Store the header in table_trace_collection only once\n",
    "table_trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"header\": list(df.columns),  # Store the header (column names)\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"\n",
    "})\n",
    "\n",
    "# Onboard data (values only, no headers)\n",
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"row_id\": index,\n",
    "        \"data\": row.tolist(),  # Store row values as a list instead of a dictionary with headers\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": ne_cols,\n",
    "            \"LIT\": lit_cols\n",
    "        },\n",
    "        \"context_columns\": [str(i) for i in range(len(df.columns))],  # Context columns (by index)\n",
    "        \"correct_qids\": {},  # Empty as GT is not available\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    input_collection.insert_one(document)\n",
    "\n",
    "# Initialize dataset-level trace (if not done earlier)\n",
    "dataset_trace_collection.update_one(\n",
    "    {\"dataset_name\": dataset_name},\n",
    "    {\n",
    "        \"$setOnInsert\": {\n",
    "            \"total_tables\": 1,  # Total number of tables\n",
    "            \"processed_tables\": 0,\n",
    "            \"total_rows\": len(df),  # This will be updated after processing\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }\n",
    "    },\n",
    "    upsert=True\n",
    ")\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")\n",
    "\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    table_trace_collection_name=\"table_trace\",\n",
    "    dataset_trace_collection_name=\"dataset_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"],  # Access the entity retrieval token directly from environment variables\n",
    "    max_workers=8,\n",
    "    candidate_retrieval_limit=10,\n",
    "    model_path=\"./training/trained_models/neural_ranker.h5\"\n",
    ")\n",
    "\n",
    "# Run the entity linking process\n",
    "crocodile_instance.run()\n",
    "\n",
    "print(\"Entity linking process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad616e-e023-4163-a71d-c8cb99ad74b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
