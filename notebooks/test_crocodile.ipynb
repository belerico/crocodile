{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data onboarded successfully for dataset 'movie_dataset' and table 'movies_table'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Crocodile.run() got an unexpected keyword argument 'dataset_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 72\u001b[0m\n\u001b[1;32m     60\u001b[0m crocodile_instance \u001b[38;5;241m=\u001b[39m Crocodile(\n\u001b[1;32m     61\u001b[0m     mongo_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmongodb://mongodb:27017/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     62\u001b[0m     db_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrocodile_db\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     entity_retrieval_token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENTITY_RETRIEVAL_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Access the entity retrieval token directly from environment variables\u001b[39;00m\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Run the entity linking process\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mcrocodile_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntity linking process completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Crocodile.run() got an unexpected keyword argument 'dataset_name'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from crocodile import Crocodile\n",
    "import os\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'MovieTitle': ['Batman Begins', 'The Dark Knight', 'Inception'],\n",
    "    'Year': [2005, 2008, 2010],\n",
    "    'Genre': ['Action', 'Action', 'Sci-Fi'],\n",
    "    'Director': ['Christopher Nolan', 'Christopher Nolan', 'Christopher Nolan']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "collection = db[\"input_data\"]\n",
    "trace_collection = db[\"processing_trace\"]\n",
    "\n",
    "# Dataset and table names for tracing\n",
    "dataset_name = \"movie_dataset\"\n",
    "table_name = \"movies_table\"\n",
    "\n",
    "# **Store the header separately** in the trace collection\n",
    "trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"header\": list(df.columns),  # Store the header (column names)\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"  # Initial status before processing\n",
    "})\n",
    "\n",
    "# Onboard data without headers, just the row values\n",
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"row_id\": index,\n",
    "        \"data\": row.tolist(),  # Store the row as a list of values\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": {  # Specify the NE columns with the correct data types\n",
    "                \"0\": \"PERSON\"  # MovieTitle considered as PERSON (e.g., protagonist/character/entity)\n",
    "            },\n",
    "            \"LIT\": {  # Specify literal columns with their data types\n",
    "                \"1\": \"NUMBER\",  # Year is a number\n",
    "                \"2\": \"STRING\"  # Genre is a string\n",
    "            }\n",
    "        },\n",
    "        \"context_columns\": [\"0\", \"1\", \"2\", \"3\"],  # Context columns (by index)\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    collection.insert_one(document)\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")\n",
    "\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    table_trace_collection_name=\"table_trace\",\n",
    "    dataset_trace_collection_name=\"dataset_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"]  # Access the entity retrieval token directly from environment variables\n",
    ")\n",
    "\n",
    "# Run the entity linking process\n",
    "crocodile_instance.run(dataset_name=dataset_name, table_name=table_name)\n",
    "\n",
    "print(\"Entity linking process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "file_path = './film_input_no_QIDs.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './film_input_no_QIDs.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "collection = db[\"input_data\"]\n",
    "trace_collection = db[\"processing_trace\"]\n",
    "\n",
    "# Dataset and table names for tracing\n",
    "dataset_name = \"imdb_dataset\"\n",
    "table_name = \"film_input_no_QIDs10\"\n",
    "\n",
    "# Onboard data\n",
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"row_id\": index,\n",
    "        \"data\": row.to_dict(),\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": [\"title\", \"director\", \"domestic distributor\"],  # Assuming Series_Title is the column to be linked\n",
    "            \"LIT\": [\"release year\", \"length in min\", \"worldwide gross\"]  # Assuming these are literal columns\n",
    "        },\n",
    "        \"context_columns\": ['title', 'director', 'release year', 'domestic distributor', 'length in min', 'worldwide gross'],  # Context columns\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    collection.insert_one(document)\n",
    "\n",
    "    #if index == 9:\n",
    "    #    break\n",
    "\n",
    "# Initialize the trace collection\n",
    "trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"  # Initial status before processing\n",
    "})\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './imdb_top_1000.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "collection = db[\"input_data\"]\n",
    "trace_collection = db[\"processing_trace\"]\n",
    "\n",
    "# Dataset and table names for tracing\n",
    "dataset_name = \"imdb_dataset\"\n",
    "table_name = \"top_1000_movies\"\n",
    "\n",
    "# Onboard data\n",
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"row_id\": index,\n",
    "        \"data\": row.to_dict(),\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": [\"Series_Title\"],  # Assuming Series_Title is the column to be linked\n",
    "            \"LIT\": [\"Released_Year\", \"Genre\"]  # Assuming these are literal columns\n",
    "        },\n",
    "        \"context_columns\": [\"Series_Title\", \"Released_Year\", \"Genre\", \"Director\"],  # Context columns\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    collection.insert_one(document)\n",
    "\n",
    "    if index == 9:\n",
    "        break\n",
    "\n",
    "# Initialize the trace collection\n",
    "trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"  # Initial status before processing\n",
    "})\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crocodile import Crocodile\n",
    "import os\n",
    "\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    collection_name=\"input_data\",\n",
    "    trace_collection_name=\"processing_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"]  # Access the entity retrieval token directly from environment variables\n",
    ")\n",
    "\n",
    "# Run the entity linking process\n",
    "crocodile_instance.run(dataset_name=dataset_name, table_name=table_name)\n",
    "\n",
    "print(\"Entity linking process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crocodile import Crocodile\n",
    "import os\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    collection_name=\"input_data\",\n",
    "    trace_collection_name=\"processing_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"\n",
    "    ]  # Access the entity retrieval token directly from environment variables\n",
    ")\n",
    "#crocodile_instance.get_bow_from_api([\"Q90\"])\n",
    "candidates = crocodile_instance.fetch_candidates(\"paris\", \"paris france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crocodile_instance.get_bow_from_api([\"Q30\", \"Q40\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_text =  \"paris france, Canadian\"\n",
    "description = candidates[0][\"description\"]\n",
    "candidate_tokens = set(crocodile_instance.tokenize_text(description))\n",
    "row_tokens = set(crocodile_instance.tokenize_text(row_text))\n",
    "crocodile_instance.calculate_token_overlap(candidate_tokens, row_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_tokens, row_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Global stopwords to avoid reinitializing repeatedly\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to get BoW vectors from the API\n",
    "def get_bow_from_api(qids):\n",
    "    url = 'https://lamapi.hel.sintef.cloud/entity/bow?token=lamapi_demo_2023'\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        headers={'accept': 'application/json', 'Content-Type': 'application/json'},\n",
    "        json={\"json\": qids}\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching BoW: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    bow_data = response.json()\n",
    "    \n",
    "    # Decode and decompress the encoded BoW vectors\n",
    "    decoded_vectors = {}\n",
    "    for qid, encoded_data in bow_data.items():\n",
    "        compressed_bytes = base64.b64decode(encoded_data)\n",
    "        decompressed_vector = pickle.loads(gzip.decompress(compressed_bytes))\n",
    "        bow_vector = decompressed_vector\n",
    "        decoded_vectors[qid] = bow_vector\n",
    "    \n",
    "    return decoded_vectors\n",
    "\n",
    "# Function to tokenize text and remove stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "\n",
    "# Function to build a row vector (BoW)\n",
    "def build_row_vector(row_text, shared_vocab):\n",
    "    row_tokens = tokenize_text(row_text)\n",
    "    row_bow = Counter(row_tokens)\n",
    "    \n",
    "    # Create a vector for the row based on the shared vocabulary\n",
    "    row_bow_vector = np.array([row_bow.get(word, 0) for word in shared_vocab])\n",
    "    \n",
    "    return row_bow_vector\n",
    "\n",
    "# Function to compute cosine similarity between row and candidate vectors\n",
    "def compute_similarity(row_bow_vector, candidate_vectors, shared_vocab):\n",
    "    similarities = {}\n",
    "    for qid, candidate_bow in candidate_vectors.items():\n",
    "        candidate_bow_vector = np.array([candidate_bow.get(word, 0) for word in shared_vocab])\n",
    "        similarity = cosine_similarity([row_bow_vector], [candidate_bow_vector])[0][0]\n",
    "        similarities[qid] = similarity\n",
    "    return similarities\n",
    "\n",
    "# Test case: simulate a row of data (e.g., a table row)\n",
    "row = {\n",
    "    'city': 'Paris',\n",
    "    'country': 'France',\n",
    "    'continent': 'Europe',\n",
    "    'population': '2140526',\n",
    "    'area_km2': '105.4',\n",
    "    'language': 'French'\n",
    "}\n",
    "\n",
    "row = {\n",
    "    'Series_Title': 'Pulp Fiction',\n",
    "    'Released_Year': 1994,\n",
    "    'Runtime (min)': 154,\n",
    "    'Genre': 'Crime, Drama',\n",
    "    'IMDB_Rating': 8.9,\n",
    "    'Overview': 'The lives of two mob hitmen, a boxer, a gangster and his wife, and a pair of diner bandits intertwine in four tales of violence and redemption.',\n",
    "    'Meta_score': 94.0,\n",
    "    'Director': 'Quentin Tarantino',\n",
    "    'Star1': 'John Travolta',\n",
    "    'No_of_Votes': 1826188,\n",
    "    'Gross': '107,928,762'\n",
    "}\n",
    "\n",
    "# Combine the row data into a single text string for BoW processing\n",
    "row_text = ' '.join([str(row[index]) for index in row if index != 'Overview'])\n",
    "\n",
    "# Step 1: Retrieve BoW vectors from API for some QIDs\n",
    "qids = [\"Q30\", \"Q166262\", \"Q90\", \"Q104123\", \"Q45\", \"Q100\", \"Q5\"]  # Example QIDs\n",
    "candidate_vectors = get_bow_from_api(qids)\n",
    "\n",
    "if candidate_vectors is None:\n",
    "    print(\"No candidate vectors retrieved from the API.\")\n",
    "else:\n",
    "    # Step 2: Ensure consistent shared vocabulary\n",
    "    shared_vocab = set()\n",
    "    for vector in candidate_vectors.values():\n",
    "        shared_vocab.update(vector.keys())  # Collect vocabulary from candidate BoWs\n",
    "    row_tokens = tokenize_text(row_text)\n",
    "    shared_vocab.update(row_tokens)\n",
    "    shared_vocab = list(shared_vocab)\n",
    "\n",
    "    # Step 3: Build the row vector (BoW)\n",
    "    row_bow_vector = build_row_vector(row_text, shared_vocab)\n",
    "\n",
    "    # Step 4: Compute similarity between row and candidate vectors\n",
    "    similarity_scores = compute_similarity(row_bow_vector, candidate_vectors, shared_vocab)\n",
    "\n",
    "    # Step 5: Output the similarity scores\n",
    "    print(\"\\nSimilarity Scores between row and candidates:\")\n",
    "    for qid, score in similarity_scores.items():\n",
    "        print(f\"QID: {qid}, Similarity: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './imdb_top_1000.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "collection = db[\"input_data\"]\n",
    "trace_collection = db[\"processing_trace\"]\n",
    "results = collection.find({})\n",
    "outcome = []\n",
    "for result in results:\n",
    "    outcome.append((result[\"row_id\"], result[\"el_results\"][\"title\"][0][\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"film_with_QIDs.csv\") \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(outcome)\n",
    "sum(df2[1] == df[\"Title_QID\"]) / len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './0DO2KMKV.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "collection = db[\"input_data\"]\n",
    "trace_collection = db[\"processing_trace\"]\n",
    "\n",
    "# Dataset and table names for tracing\n",
    "dataset_name = \"test\"\n",
    "table_name = \"0DO2KMKV\"\n",
    "\n",
    "# Onboard data\n",
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"row_id\": index,\n",
    "        \"data\": row.to_dict(),\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": [\"col0\"],  # Assuming Series_Title is the column to be linked\n",
    "            \"LIT\": [\"col1\"]  # Assuming these are literal columns\n",
    "        },\n",
    "        \"context_columns\": list(df.columns),  # Context columns\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    collection.insert_one(document)\n",
    "\n",
    "    #if index == 9:\n",
    "    #    break\n",
    "\n",
    "# Initialize the trace collection\n",
    "trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"  # Initial status before processing\n",
    "})\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crocodile import Crocodile\n",
    "import os\n",
    "\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    collection_name=\"input_data\",\n",
    "    trace_collection_name=\"processing_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"]  # Access the entity retrieval token directly from environment variables\n",
    ")\n",
    "\n",
    "# Run the entity linking process\n",
    "crocodile_instance.run(dataset_name=dataset_name, table_name=table_name)\n",
    "\n",
    "print(\"Entity linking process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from crocodile import Crocodile\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './tables/VGUZX5R3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "\n",
    "dataset_name = \"test\"\n",
    "table_name = \"VGUZX5R3\"\n",
    "\n",
    "# Load the correct QIDs for the table\n",
    "with open('./tables/correct_qids_VGUZX5R3.json', 'r') as file:\n",
    "    correct_qids = json.load(file)\n",
    "\n",
    "# **Store the header only once** in the table_trace_collection\n",
    "table_trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"header\": list(df.columns),  # Store the header (column names)\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"\n",
    "})\n",
    "\n",
    "# Onboard data (values only, no headers)\n",
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name, \n",
    "        \"row_id\": index,\n",
    "        \"data\": row.tolist(),  # Store row values as a list instead of a dictionary with headers\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": {\n",
    "                \"0\": \"LOCATION\",\n",
    "                \"1\": \"LOCATION\",\n",
    "                \"3\": \"LOCATION\"\n",
    "            },\n",
    "            \"LIT\": {\n",
    "                \"2\": \"NUMBER\", \n",
    "                \"4\": \"NUMBER\", \n",
    "                \"5\": \"NUMBER\"\n",
    "            }\n",
    "        },\n",
    "        \"context_columns\": [str(i) for i in range(len(df.columns))],  # Context columns (by index)\n",
    "        \"correct_qids\": correct_qids,\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    input_collection.insert_one(document)\n",
    "\n",
    "# Initialize dataset-level trace (if not done earlier)\n",
    "dataset_trace_collection.update_one(\n",
    "    {\"dataset_name\": dataset_name},\n",
    "    {\n",
    "        \"$setOnInsert\": {\n",
    "            \"total_tables\": 1,  # Total number of tables\n",
    "            \"processed_tables\": 0,\n",
    "            \"total_rows\": 0,  # This will be updated after processing\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }\n",
    "    },\n",
    "    upsert=True\n",
    ")\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")\n",
    "\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    table_trace_collection_name=\"table_trace\",\n",
    "    dataset_trace_collection_name=\"dataset_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"]  # Access the entity retrieval token directly from environment variables\n",
    ")\n",
    "\n",
    "# Run the entity linking process\n",
    "crocodile_instance.run(dataset_name=dataset_name, table_name=table_name)\n",
    "\n",
    "print(\"Entity linking process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from crocodile import Crocodile\n",
    "import os\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './tables/imdb_top_1000.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "process_queue = db[\"process_queue\"]\n",
    "\n",
    "dataset_name = \"test\"\n",
    "table_name = \"imdb_top_1000_speed_test1\"\n",
    "\n",
    "\n",
    "# Ensure indexes for uniqueness and performance\n",
    "def ensure_indexes():\n",
    "    input_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING), (\"row_id\", ASCENDING)], unique=True)\n",
    "    table_trace_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    dataset_trace_collection.create_index([(\"dataset_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"status\", ASCENDING)])  # Ensure fast retrieval of items by status\n",
    "\n",
    "ensure_indexes()\n",
    "\n",
    "# Define column classifications for NE and LIT types\n",
    "ne_cols = {\n",
    "    \"0\": \"TITLE\",    # Series_Title\n",
    "    \"7\": \"PERSON\",   # Director\n",
    "    \"8\": \"PERSON\"    # Star1\n",
    "}\n",
    "\n",
    "lit_cols = {\n",
    "    \"1\": \"NUMBER\",   # Released_Year\n",
    "    \"2\": \"NUMBER\",   # Runtime (min)\n",
    "    \"3\": \"STRING\",    # Genre\n",
    "    \"4\": \"NUMBER\",   # IMDB_Rating\n",
    "    \"5\": \"STRING\",   # Overview\n",
    "    \"6\": \"NUMBER\",   # Meta_score\n",
    "    \"9\": \"NUMBER\",   # No_of_Votes\n",
    "    \"10\": \"NUMBER\"   # Gross\n",
    "}\n",
    "\n",
    "# Store the header in table_trace_collection only once\n",
    "table_trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"header\": list(df.columns),  # Store the header (column names)\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"\n",
    "})\n",
    "\n",
    "# Onboard data (values only, no headers)\n",
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"row_id\": index,\n",
    "        \"data\": row.tolist(),  # Store row values as a list instead of a dictionary with headers\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": ne_cols,\n",
    "            \"LIT\": lit_cols\n",
    "        },\n",
    "        \"context_columns\": [str(i) for i in range(len(df.columns))],  # Context columns (by index)\n",
    "        \"correct_qids\": {},  # Empty as GT is not available\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    input_collection.insert_one(document)\n",
    "\n",
    "# Initialize dataset-level trace (if not done earlier)\n",
    "dataset_trace_collection.update_one(\n",
    "    {\"dataset_name\": dataset_name},\n",
    "    {\n",
    "        \"$setOnInsert\": {\n",
    "            \"total_tables\": 1,  # Total number of tables\n",
    "            \"processed_tables\": 0,\n",
    "            \"total_rows\": 0,  # This will be updated after processing\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }\n",
    "    },\n",
    "    upsert=True\n",
    ")\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")\n",
    "\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    table_trace_collection_name=\"table_trace\",\n",
    "    dataset_trace_collection_name=\"dataset_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"]  # Access the entity retrieval token directly from environment variables\n",
    ")\n",
    "\n",
    "# Run the entity linking process\n",
    "crocodile_instance.run(dataset_name=dataset_name, table_name=table_name)\n",
    "\n",
    "print(\"Entity linking process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from crocodile import Crocodile\n",
    "import os\n",
    "import json\n",
    "#from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './tables/VGUZX5R3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "# Drop the entire crocodile_db database\n",
    "client.drop_database(\"crocodile_db\")\n",
    "print(\"The crocodile_db database has been dropped.\")\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "\n",
    "dataset_name = \"test\"\n",
    "table_name = \"VGUZX5R3\"\n",
    "\n",
    "# Load the correct QIDs for the table\n",
    "with open('./tables/correct_qids_VGUZX5R3.json', 'r') as file:\n",
    "    correct_qids_data = json.load(file)\n",
    "\n",
    "# Store the header only once in the table_trace_collection\n",
    "table_trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"header\": list(df.columns),  # Store the header (column names)\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"\n",
    "})\n",
    "\n",
    "# Onboard data (values only, no headers)\n",
    "for index, row in df.iterrows():\n",
    "    # Filter correct QIDs relevant for the current row\n",
    "    correct_qids_for_row = {key: value for key, value in correct_qids_data.items() if key.startswith(f\"{index}-\")}\n",
    "    \n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name, \n",
    "        \"row_id\": index,\n",
    "        \"data\": row.tolist(),  # Store row values as a list instead of a dictionary with headers\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": {\n",
    "                \"0\": \"LOCATION\",\n",
    "                \"1\": \"LOCATION\",\n",
    "                \"3\": \"LOCATION\"\n",
    "            },\n",
    "            \"LIT\": {\n",
    "                \"2\": \"NUMBER\", \n",
    "                \"4\": \"NUMBER\", \n",
    "                \"5\": \"NUMBER\"\n",
    "            }\n",
    "        },\n",
    "        \"context_columns\": [str(i) for i in range(len(df.columns))],  # Context columns (by index)\n",
    "        \"correct_qids\": correct_qids_for_row,  # Only relevant QIDs for the current row\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    input_collection.insert_one(document)\n",
    "\n",
    "# Initialize dataset-level trace (if not done earlier)\n",
    "dataset_trace_collection.update_one(\n",
    "    {\"dataset_name\": dataset_name},\n",
    "    {\n",
    "        \"$setOnInsert\": {\n",
    "            \"total_tables\": 1,  # Total number of tables\n",
    "            \"processed_tables\": 0,\n",
    "            \"total_rows\": 0,  # This will be updated after processing\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }\n",
    "    },\n",
    "    upsert=True\n",
    ")\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")\n",
    "\n",
    "model_path = \"./training/trained_models/neural_ranker.h5\"\n",
    "#ml_model = load_model(model_path)\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    table_trace_collection_name=\"table_trace\",\n",
    "    dataset_trace_collection_name=\"dataset_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"],  # Access the entity retrieval token directly from environment variables\n",
    "    candidate_retrieval_limit=10,\n",
    "    max_workers=30,\n",
    "    model_path=model_path\n",
    ")\n",
    "\n",
    "# Run the entity linking process\n",
    "crocodile_instance.run(dataset_name=dataset_name, table_name=table_name)\n",
    "\n",
    "print(\"Entity linking process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from crocodile import Crocodile\n",
    "import os\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './tables/imdb_top_1000.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "# Drop the entire crocodile_db database\n",
    "client.drop_database(\"crocodile_db\")\n",
    "print(\"The crocodile_db database has been dropped.\")\n",
    "\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "process_queue = db[\"process_queue\"]\n",
    "\n",
    "dataset_name = \"test\"\n",
    "table_name = \"imdb_top_1000_speed_test\"\n",
    "\n",
    "\n",
    "# Ensure indexes for uniqueness and performance\n",
    "def ensure_indexes():\n",
    "    input_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING), (\"row_id\", ASCENDING)], unique=True)\n",
    "    table_trace_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    dataset_trace_collection.create_index([(\"dataset_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"status\", ASCENDING)])  # Ensure fast retrieval of items by status\n",
    "\n",
    "ensure_indexes()\n",
    "\n",
    "# Define column classifications for NE and LIT types\n",
    "ne_cols = {\n",
    "    \"0\": \"TITLE\",    # Series_Title\n",
    "    \"7\": \"PERSON\",   # Director\n",
    "    \"8\": \"PERSON\"    # Star1\n",
    "}\n",
    "\n",
    "lit_cols = {\n",
    "    \"1\": \"NUMBER\",   # Released_Year\n",
    "    \"2\": \"NUMBER\",   # Runtime (min)\n",
    "    \"3\": \"STRING\",    # Genre\n",
    "    \"4\": \"NUMBER\",   # IMDB_Rating\n",
    "    \"5\": \"STRING\",   # Overview\n",
    "    \"6\": \"NUMBER\",   # Meta_score\n",
    "    \"9\": \"NUMBER\",   # No_of_Votes\n",
    "    \"10\": \"NUMBER\"   # Gross\n",
    "}\n",
    "\n",
    "# Store the header in table_trace_collection only once\n",
    "table_trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"header\": list(df.columns),  # Store the header (column names)\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"\n",
    "})\n",
    "\n",
    "# Onboard data (values only, no headers)\n",
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"row_id\": index,\n",
    "        \"data\": row.tolist(),  # Store row values as a list instead of a dictionary with headers\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": ne_cols,\n",
    "            \"LIT\": lit_cols\n",
    "        },\n",
    "        \"context_columns\": [str(i) for i in range(len(df.columns))],  # Context columns (by index)\n",
    "        \"correct_qids\": {},  # Empty as GT is not available\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    input_collection.insert_one(document)\n",
    "\n",
    "# Initialize dataset-level trace (if not done earlier)\n",
    "dataset_trace_collection.update_one(\n",
    "    {\"dataset_name\": dataset_name},\n",
    "    {\n",
    "        \"$setOnInsert\": {\n",
    "            \"total_tables\": 1,  # Total number of tables\n",
    "            \"processed_tables\": 0,\n",
    "            \"total_rows\": 0,  # This will be updated after processing\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }\n",
    "    },\n",
    "    upsert=True\n",
    ")\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")\n",
    "\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    table_trace_collection_name=\"table_trace\",\n",
    "    dataset_trace_collection_name=\"dataset_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"],  # Access the entity retrieval token directly from environment variables\n",
    "    max_workers=32,\n",
    "    candidate_retrieval_limit=10,\n",
    "    model_path=\"./training/trained_models/neural_ranker.h5\"\n",
    ")\n",
    "\n",
    "# Run the entity linking process\n",
    "crocodile_instance.run()\n",
    "\n",
    "print(\"Entity linking process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from crocodile import Crocodile\n",
    "import os\n",
    "\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    table_trace_collection_name=\"table_trace\",\n",
    "    dataset_trace_collection_name=\"dataset_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"],  # Access the entity retrieval token directly from environment variables\n",
    "    max_workers=50,\n",
    "    candidate_retrieval_limit=10,\n",
    "    model_path=\"./training/trained_models/neural_ranker.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crocodile_instance.get_bow_from_api(\"United States Washington D.C. 331000000 North America\", [\"Q30\", \"Q32\", \"Q35\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Define the curl command\n",
    "curl_command = [\n",
    "    \"curl\", \"-X\", \"POST\",\n",
    "    \"https://lamapi.hel.sintef.cloud/entity/bow?token=lamapi_demo_2023\",\n",
    "    \"-H\", \"accept: application/json\",\n",
    "    \"-H\", \"Content-Type: application/json\",\n",
    "    \"-d\", '''\n",
    "    {\n",
    "      \"json\": {\n",
    "        \"text\": \"galaxy supernova remnant emission nebula\",\n",
    "        \"qids\": [\"Q3094537\", \"Q1249631\", \"Q58030121\", \"Q56322597\", \"Q112178715\", \"Q3094522\", \"Q3038279\", \"Q3757513\", \"Q375383\", \"Q111055666\", \"Q318\"]\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "]\n",
    "\n",
    "# Measure the time taken\n",
    "start_time = time.time()\n",
    "result = subprocess.run(curl_command, capture_output=True, text=True)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the output and time taken\n",
    "print(\"Response:\")\n",
    "print(result.stdout)\n",
    "print(\"\\nTime taken (seconds):\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Define the curl command\n",
    "curl_command = [\n",
    "    \"curl\", \"-X\", \"POST\",\n",
    "    \"https://lamapi.hel.sintef.cloud/entity/bow?token=lamapi_demo_2023\",\n",
    "    \"-H\", \"accept: application/json\",\n",
    "    \"-H\", \"Content-Type: application/json\",\n",
    "    \"-d\", '''\n",
    "    {\n",
    "      \"json\": {\n",
    "        \"text\": \"open cluster star cluster galaxy\",\n",
    "        \"qids\": [\"Q3094537\", \"Q1249631\", \"Q58030121\", \"Q56322597\", \"Q112178715\", \"Q3094522\", \"Q3038279\", \"Q3757513\", \"Q375383\", \"Q111055666\", \"Q318\"]\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "]\n",
    "\n",
    "# Measure the time taken\n",
    "start_time = time.time()\n",
    "result = subprocess.run(curl_command, capture_output=True, text=True)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the output and time taken\n",
    "print(\"Response:\")\n",
    "print(result.stdout)\n",
    "print(\"\\nTime taken (seconds):\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 23:25:05.677339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped collection: process_queue\n",
      "Dropped collection: input_data\n",
      "Dropped collection: timing_trace\n",
      "Dropped collection: training_data\n",
      "Dropped collection: dataset_trace\n",
      "Dropped collection: table_trace\n",
      "All unwanted collections have been dropped.\n",
      "Data onboarded successfully for dataset 'test' and table 'imdb_top_1000_speed_test'.\n",
      "Found 1000 tasks to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 23:25:07.771382: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 23:25:07.771686: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.775133: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.810419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 23:25:07.859375: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 23:25:07.860053: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.863088: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.893777: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 23:25:07.894341: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.895348: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 23:25:07.899300: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.912665: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 23:25:07.913038: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.915725: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.935323: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 23:25:07.935844: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.940560: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.946873: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 23:25:07.947239: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.950494: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:07.955518: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 23:25:07.970725: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 23:25:07.983632: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 23:25:08.005902: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 23:25:08.006424: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:08.011304: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:08.013539: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 23:25:08.087291: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 23:25:08.139912: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 23:25:08.140308: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:08.143312: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:08.182239: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 23:25:08.242526: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 23:25:08.243284: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:08.248456: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-15 23:25:08.326321: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-15 23:25:08.508767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-12-15 23:25:08.609308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-12-15 23:25:08.808946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-12-15 23:25:09.073189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-12-15 23:25:09.183052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-12-15 23:25:09.187040: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-12-15 23:25:09.231583: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-12-15 23:25:09.245654: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-12-15 23:25:09.334574: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unprocessed documents: 1037\n",
      "Predicting scores for 30322 candidates...\n",
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "No more tasks to process.\n",
      "Scores predicted.\n",
      "ML ranking progress: 98.75% completed\n",
      "Predicting scores for 9897 candidates...\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Scores predicted.\n",
      "ML ranking progress: 100.00% completed\n",
      "ML ranking completed.\n",
      "All tasks have been processed.\n",
      "Entity linking process completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from crocodile import Crocodile\n",
    "import os\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './tables/imdb_top_1000.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "# Drop the entire crocodile_db database\n",
    "#client.drop_database(\"crocodile_db\")\n",
    "db = client[\"crocodile_db\"]\n",
    "\n",
    "# Drop all collections except 'bow_cache' and 'candidate_cache'\n",
    "collections_to_keep = [\"bow_cache\", \"candidate_cache\"]\n",
    "all_collections = db.list_collection_names()\n",
    "\n",
    "for collection in all_collections:\n",
    "    if collection not in collections_to_keep:\n",
    "        db[collection].drop()\n",
    "        print(f\"Dropped collection: {collection}\")\n",
    "\n",
    "print(\"All unwanted collections have been dropped.\")\n",
    "\n",
    "\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "process_queue = db[\"process_queue\"]\n",
    "\n",
    "dataset_name = \"test\"\n",
    "table_name = \"imdb_top_1000_speed_test\"\n",
    "\n",
    "\n",
    "# Ensure indexes for uniqueness and performance\n",
    "def ensure_indexes():\n",
    "    input_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING), (\"row_id\", ASCENDING)], unique=True)\n",
    "    table_trace_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    dataset_trace_collection.create_index([(\"dataset_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "    process_queue.create_index([(\"status\", ASCENDING)])  # Ensure fast retrieval of items by status\n",
    "\n",
    "ensure_indexes()\n",
    "\n",
    "# Define column classifications for NE and LIT types\n",
    "ne_cols = {\n",
    "    \"0\": \"OTHER\",    # Series_Title\n",
    "    \"7\": \"PERSON\",   # Director\n",
    "    \"8\": \"PERSON\"    # Star1\n",
    "}\n",
    "\n",
    "lit_cols = {\n",
    "    \"1\": \"NUMBER\",   # Released_Year\n",
    "    \"2\": \"NUMBER\",   # Runtime (min)\n",
    "    \"3\": \"STRING\",    # Genre\n",
    "    \"4\": \"NUMBER\",   # IMDB_Rating\n",
    "    \"5\": \"STRING\",   # Overview\n",
    "    \"6\": \"NUMBER\",   # Meta_score\n",
    "    \"9\": \"NUMBER\",   # No_of_Votes\n",
    "    \"10\": \"NUMBER\"   # Gross\n",
    "}\n",
    "\n",
    "# Store the header in table_trace_collection only once\n",
    "table_trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"header\": list(df.columns),  # Store the header (column names)\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"\n",
    "})\n",
    "\n",
    "# Onboard data (values only, no headers)\n",
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"row_id\": index,\n",
    "        \"data\": row.tolist(),  # Store row values as a list instead of a dictionary with headers\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": ne_cols,\n",
    "            \"LIT\": lit_cols\n",
    "        },\n",
    "        \"context_columns\": [str(i) for i in range(len(df.columns))],  # Context columns (by index)\n",
    "        \"correct_qids\": {},  # Empty as GT is not available\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    input_collection.insert_one(document)\n",
    "\n",
    "# Initialize dataset-level trace (if not done earlier)\n",
    "dataset_trace_collection.update_one(\n",
    "    {\"dataset_name\": dataset_name},\n",
    "    {\n",
    "        \"$setOnInsert\": {\n",
    "            \"total_tables\": 1,  # Total number of tables\n",
    "            \"processed_tables\": 0,\n",
    "            \"total_rows\": 0,  # This will be updated after processing\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }\n",
    "    },\n",
    "    upsert=True\n",
    ")\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")\n",
    "\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    table_trace_collection_name=\"table_trace\",\n",
    "    dataset_trace_collection_name=\"dataset_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_bow_endpoint=os.environ[\"ENTITY_BOW_ENDPOINT\"],  # Access the entity BoW endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"],  # Access the entity retrieval token directly from environment variables\n",
    "    max_workers=8,\n",
    "    candidate_retrieval_limit=10,\n",
    "    model_path=\"./training/trained_models/neural_ranker.h5\"\n",
    ")\n",
    "\n",
    "# Run the entity linking process\n",
    "crocodile_instance.run()\n",
    "\n",
    "print(\"Entity linking process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
