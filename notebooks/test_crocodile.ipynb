{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/crocodile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data onboarded successfully for dataset 'movie_dataset' and table 'movies_table'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'MovieTitle': ['Batman Begins', 'The Dark Knight', 'Inception'],\n",
    "    'Year': [2005, 2008, 2010],\n",
    "    'Genre': ['Action', 'Action', 'Sci-Fi'],\n",
    "    'Director': ['Christopher Nolan', 'Christopher Nolan', 'Christopher Nolan']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "collection = db[\"input_data\"]\n",
    "trace_collection = db[\"processing_trace\"]\n",
    "\n",
    "# Dataset and table names for tracing\n",
    "dataset_name = \"movie_dataset\"\n",
    "table_name = \"movies_table\"\n",
    "\n",
    "# Onboard data\n",
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"row_id\": index,\n",
    "        \"data\": row.to_dict(),\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": [\"MovieTitle\"],  # Specify columns to be linked\n",
    "            \"LIT\": [\"Year\", \"Genre\"]  # Specify literal columns\n",
    "        },\n",
    "        \"context_columns\": [\"MovieTitle\", \"Year\", \"Genre\", \"Director\"],  # Specify context columns\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    collection.insert_one(document)\n",
    "\n",
    "# Initialize the trace collection\n",
    "trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"  # Initial status before processing\n",
    "})\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data onboarded successfully for dataset 'imdb_dataset' and table 'top_1000_movies'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = './imdb_top_1000.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "collection = db[\"input_data\"]\n",
    "trace_collection = db[\"processing_trace\"]\n",
    "\n",
    "# Dataset and table names for tracing\n",
    "dataset_name = \"imdb_dataset\"\n",
    "table_name = \"top_1000_movies\"\n",
    "\n",
    "# Onboard data\n",
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"table_name\": table_name,\n",
    "        \"row_id\": index,\n",
    "        \"data\": row.to_dict(),\n",
    "        \"classified_columns\": {\n",
    "            \"NE\": [\"Series_Title\"],  # Assuming Series_Title is the column to be linked\n",
    "            \"LIT\": [\"Released_Year\", \"Genre\"]  # Assuming these are literal columns\n",
    "        },\n",
    "        \"context_columns\": [\"Series_Title\", \"Released_Year\", \"Genre\", \"Director\"],  # Context columns\n",
    "        \"status\": \"TODO\"\n",
    "    }\n",
    "    collection.insert_one(document)\n",
    "\n",
    "# Initialize the trace collection\n",
    "trace_collection.insert_one({\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"table_name\": table_name,\n",
    "    \"total_rows\": len(df),\n",
    "    \"processed_rows\": 0,\n",
    "    \"status\": \"PENDING\"  # Initial status before processing\n",
    "})\n",
    "\n",
    "print(f\"Data onboarded successfully for dataset '{dataset_name}' and table '{table_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity linking process completed.\n"
     ]
    }
   ],
   "source": [
    "from crocodile import Crocodile\n",
    "import os\n",
    "\n",
    "# Create an instance of the Crocodile class\n",
    "crocodile_instance = Crocodile(\n",
    "    mongo_uri=\"mongodb://mongodb:27017/\",\n",
    "    db_name=\"crocodile_db\",\n",
    "    collection_name=\"input_data\",\n",
    "    trace_collection_name=\"processing_trace\",\n",
    "    max_candidates=3,\n",
    "    entity_retrieval_endpoint=os.environ[\"ENTITY_RETRIEVAL_ENDPOINT\"],  # Access the entity retrieval endpoint directly from environment variables\n",
    "    entity_retrieval_token=os.environ[\"ENTITY_RETRIEVAL_TOKEN\"]  # Access the entity retrieval token directly from environment variables\n",
    ")\n",
    "\n",
    "# Run the entity linking process\n",
    "crocodile_instance.run(dataset_name=dataset_name, table_name=table_name)\n",
    "\n",
    "print(\"Entity linking process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Levenshtein\n",
      "  Downloading Levenshtein-0.25.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.3 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein)\n",
      "  Downloading rapidfuzz-3.9.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (12 kB)\n",
      "Downloading Levenshtein-0.25.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (169 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.5/169.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.9.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
      "Successfully installed Levenshtein-0.25.1 rapidfuzz-3.9.6\n"
     ]
    }
   ],
   "source": [
    "! pip install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
