{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables...: 100%|██████████| 64/64 [00:00<00:00, 302.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3\", \"2T_2020\", \"Round4\", \"HardTableR2\", \"HardTableR3\"]\n",
    "\n",
    "\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    ne_cols = {str(col_index): None for col_index in filtered_cea_gt[2].unique()}\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{row[1]}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "    tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "    for table in tqdm(tables, desc=\"Processing tables...\"):\n",
    "        if table.endswith(\".csv\"):\n",
    "            df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "            table_name = table.split(\".csv\")[0]\n",
    "            ne_cols, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_cols, correct_qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables...:   0%|          | 0/64 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ner_response:\n\u001b[1;32m     54\u001b[0m     lit_cols \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# Initialize LIT columns dictionary\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col_idx, ner_info \u001b[38;5;129;01min\u001b[39;00m \u001b[43mner_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m     56\u001b[0m         col_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(ner_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_column\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# Get column index as string\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         classification \u001b[38;5;241m=\u001b[39m ner_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Get NER or LIT type\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3\", \"2T_2020\", \"Round4_2020\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "# Function to get NE columns and correct QIDs from the GT file\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    # Initialize ne_cols as a dict where the value (NER type) is None initially\n",
    "    ne_cols = {str(col_index): None for col_index in filtered_cea_gt[2].unique()}\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{row[1]}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "# Function to call the NER API endpoint\n",
    "def classify_columns_with_api(df, type=\"accurate\"):\n",
    "    payload = {\n",
    "        \"json\": [df.T.astype(str).values.tolist()]  # Transpose to align columns and convert to list of rows\n",
    "    }\n",
    "    url = f'https://lamapi.hel.sintef.cloud/sti/column-analysis?model_type={type}&token=lamapi_demo_2023'\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return JSON response with NER classifications\n",
    "    else:\n",
    "        print(f\"Error with API call: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main processing loop\n",
    "for dataset in datasets:\n",
    "    cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "    tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "    \n",
    "    for table in tqdm(tables, desc=\"Processing tables...\"):\n",
    "        if table.endswith(\".csv\"):\n",
    "            df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "            table_name = table.split(\".csv\")[0]\n",
    "\n",
    "            # Get NE columns and correct QIDs from GT\n",
    "            ne_cols, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "            \n",
    "            # Call the NER API to classify columns\n",
    "            ner_response = classify_columns_with_api(df)\n",
    "            if ner_response:\n",
    "                lit_cols = {}  # Initialize LIT columns dictionary\n",
    "                for col_idx, ner_info in ner_response.items():\n",
    "                    col_num = str(ner_info[\"index_column\"])  # Get column index as string\n",
    "                    classification = ner_info[\"classification\"]  # Get NER or LIT type\n",
    "                    tag = ner_info[\"tag\"]\n",
    "\n",
    "                    # Update NE columns from GT\n",
    "                    if col_num in ne_cols:\n",
    "                        # If GT classifies as NE, we trust the GT and update with the NER type\n",
    "                        if tag == \"LIT\":\n",
    "                            classification = \"OTHER\"\n",
    "                        ne_cols[col_num] = classification\n",
    "                    elif tag == \"NE\":\n",
    "                        # If API classifies as NE and not in GT, add to ne_cols\n",
    "                        ne_cols[col_num] = classification\n",
    "                    elif tag == \"LIT\":\n",
    "                        # If API classifies as LIT, add to lit_cols\n",
    "                        lit_cols[col_num] = classification\n",
    "        break\n",
    "    break  # Keep the break for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'table_1': {'0': {'index_column': 0,\n",
       "    'tag': 'LIT',\n",
       "    'classification': 'NUMBER',\n",
       "    'datatype': 'NUMBER',\n",
       "    'probabilities': {'NUMBER': 1.0}},\n",
       "   '1': {'index_column': 1,\n",
       "    'tag': 'NE',\n",
       "    'classification': 'OTHER',\n",
       "    'datatype': 'OTHER',\n",
       "    'probabilities': {'LOCATION': 0.06,\n",
       "     'PERSON': 0.02,\n",
       "     'OTHER': 0.84,\n",
       "     'STRING': 0.06}},\n",
       "   '2': {'index_column': 2,\n",
       "    'tag': 'LIT',\n",
       "    'classification': 'NUMBER',\n",
       "    'datatype': 'NUMBER',\n",
       "    'probabilities': {'NUMBER': 1.0, 'DATE': 1.0}},\n",
       "   '3': {'index_column': 3,\n",
       "    'tag': 'NE',\n",
       "    'classification': 'PERSON',\n",
       "    'datatype': 'PERSON',\n",
       "    'probabilities': {'PERSON': 1.0}},\n",
       "   '4': {'index_column': 4,\n",
       "    'tag': 'LIT',\n",
       "    'classification': 'NUMBER',\n",
       "    'datatype': 'NUMBER',\n",
       "    'probabilities': {'NUMBER': 1.0}}}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ne_cols, lit_cols, \u001b[43mtag\u001b[49m, ner_response\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tag' is not defined"
     ]
    }
   ],
   "source": [
    "ne_cols, lit_cols, tag, ner_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col0</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Somosierra</td>\n",
       "      <td>Autovía A-1</td>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fotu La</td>\n",
       "      <td>National Highway 1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zojila Pass</td>\n",
       "      <td>National Highway 1</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jablunkov Pass</td>\n",
       "      <td>European route E75</td>\n",
       "      <td>Czech Republic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wildhaus Pass</td>\n",
       "      <td>Main road 16</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pacific Grade Summit</td>\n",
       "      <td>California State Route 4</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mahoosuc Notch</td>\n",
       "      <td>Appalachian Trail</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Zealand Notch</td>\n",
       "      <td>Appalachian Trail</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Summit Pass</td>\n",
       "      <td>Alaska Highway</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Muncho Pass</td>\n",
       "      <td>Alaska Highway</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pine Pass</td>\n",
       "      <td>Canadian National Railway</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dyulino Pass</td>\n",
       "      <td>Cherno More motorway</td>\n",
       "      <td>Bulgaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Vitinya Pass</td>\n",
       "      <td>Hemus motorway</td>\n",
       "      <td>Bulgaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Gate of Trajan</td>\n",
       "      <td>Trakia motorway</td>\n",
       "      <td>Bulgaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Postojna Gate</td>\n",
       "      <td>A1 motorway</td>\n",
       "      <td>Slovenia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Packsattel</td>\n",
       "      <td>Süd Autobahn</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Satus Pass</td>\n",
       "      <td>U.S. Route 97</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Blewett Pass</td>\n",
       "      <td>U.S. Route 97</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kingman Pass</td>\n",
       "      <td>U.S. Route 89</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Vulcan Pass</td>\n",
       "      <td>Jiu River</td>\n",
       "      <td>Romania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    col0                       col1                      col2\n",
       "0             Somosierra                Autovía A-1                     Spain\n",
       "1                Fotu La         National Highway 1                     India\n",
       "2            Zojila Pass         National Highway 1                     India\n",
       "3         Jablunkov Pass         European route E75            Czech Republic\n",
       "4          Wildhaus Pass               Main road 16               Switzerland\n",
       "5   Pacific Grade Summit   California State Route 4  United States of America\n",
       "6         Mahoosuc Notch          Appalachian Trail  United States of America\n",
       "7          Zealand Notch          Appalachian Trail  United States of America\n",
       "8            Summit Pass             Alaska Highway                    Canada\n",
       "9            Muncho Pass             Alaska Highway                    Canada\n",
       "10             Pine Pass  Canadian National Railway                    Canada\n",
       "11          Dyulino Pass       Cherno More motorway                  Bulgaria\n",
       "12          Vitinya Pass             Hemus motorway                  Bulgaria\n",
       "13        Gate of Trajan            Trakia motorway                  Bulgaria\n",
       "14         Postojna Gate                A1 motorway                  Slovenia\n",
       "15            Packsattel               Süd Autobahn                   Austria\n",
       "16            Satus Pass              U.S. Route 97  United States of America\n",
       "17          Blewett Pass              U.S. Route 97  United States of America\n",
       "18          Kingman Pass              U.S. Route 89  United States of America\n",
       "19           Vulcan Pass                  Jiu River                   Romania"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables for dataset Round1_T2D...: 100%|██████████| 64/64 [00:50<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://mongodb:27017/\")\n",
    "db = client[\"crocodile_db\"]\n",
    "input_collection = db[\"input_data\"]\n",
    "table_trace_collection = db[\"table_trace\"]\n",
    "dataset_trace_collection = db[\"dataset_trace\"]\n",
    "process_queue = db[\"process_queue\"]\n",
    "\n",
    "# Ensure indexes for uniqueness and performance\n",
    "input_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING), (\"row_id\", ASCENDING)], unique=True)\n",
    "table_trace_collection.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "dataset_trace_collection.create_index([(\"dataset_name\", ASCENDING)], unique=True)\n",
    "process_queue.create_index([(\"dataset_name\", ASCENDING), (\"table_name\", ASCENDING)], unique=True)\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3\", \"2T_2020\", \"Round4_2020\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "# Function to get NE columns and correct QIDs from the GT file\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    # Initialize ne_cols as a dict where the value (NER type) is None initially\n",
    "    ne_cols = {str(col_index): None for col_index in filtered_cea_gt[2].unique()}\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{row[1]}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "# Function to call the NER API endpoint\n",
    "def classify_columns_with_api(df, type=\"accurate\"):\n",
    "    payload = {\n",
    "        \"json\": df.T.astype(str).values.tolist()  # Transpose to align columns and convert to list of rows\n",
    "    }\n",
    "    url = f'https://lamapi.hel.sintef.cloud/sti/column-analysis?model_type={type}&token=lamapi_demo_2023'\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return JSON response with NER classifications\n",
    "    else:\n",
    "        print(f\"Error with API call: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to onboard data into MongoDB\n",
    "def onboard_data(dataset_name, table_name, df, ne_cols, correct_qids, ner_response):\n",
    "    lit_cols = {}\n",
    "    \n",
    "    # Parse the API response for NE and LIT columns\n",
    "    if ner_response:\n",
    "        for col_idx, ner_info in ner_response.items():\n",
    "            col_num = str(ner_info[\"index_column\"])\n",
    "            classification = ner_info[\"classification\"]\n",
    "            tag = ner_info[\"tag\"]\n",
    "            \n",
    "            if col_num in ne_cols:\n",
    "                # If GT classifies as NE, we trust the GT and update with NER type\n",
    "                if tag == \"LIT\":\n",
    "                    classification = \"OTHER\"\n",
    "                ne_cols[col_num] = classification\n",
    "            elif tag == \"NE\":\n",
    "                # If API classifies as NE but it's not in GT, add it to NE columns\n",
    "                ne_cols[col_num] = classification\n",
    "            elif tag == \"LIT\":\n",
    "                # If API classifies as LIT, add to LIT columns\n",
    "                lit_cols[col_num] = classification\n",
    "\n",
    "    # Insert data row by row\n",
    "    for index, row in df.iterrows():\n",
    "        document = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"table_name\": table_name,\n",
    "            \"row_id\": index,\n",
    "            \"data\": row.to_dict(),\n",
    "            \"classified_columns\": {\n",
    "                \"NE\": ne_cols,  # Use updated NE columns\n",
    "                \"LIT\": lit_cols  # Use LIT columns from the API response\n",
    "            },\n",
    "            \"context_columns\": [str(i) for i in range(len(df.columns))],  # Context columns\n",
    "            \"correct_qids\": correct_qids,  # Correct QIDs from GT\n",
    "            \"status\": \"TODO\"\n",
    "        }\n",
    "\n",
    "        # Insert or update the document in MongoDB\n",
    "        input_collection.update_one(\n",
    "            {\"dataset_name\": dataset_name, \"table_name\": table_name, \"row_id\": index},\n",
    "            {\"$set\": document},\n",
    "            upsert=True\n",
    "        )\n",
    "\n",
    "    # Log onboarding completion for table-level trace\n",
    "    table_trace_collection.update_one(\n",
    "        {\"dataset_name\": dataset_name, \"table_name\": table_name},\n",
    "        {\"$set\": {\n",
    "            \"total_rows\": len(df),\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "# Main processing loop for onboarding datasets\n",
    "for dataset in datasets:\n",
    "    cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "    tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "    \n",
    "    for table in tqdm(tables, desc=f\"Processing tables for dataset {dataset}...\"):\n",
    "        if table.endswith(\".csv\"):\n",
    "            df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "            table_name = table.split(\".csv\")[0]\n",
    "\n",
    "            # Get NE columns and correct QIDs from GT\n",
    "            ne_cols, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "            \n",
    "            # Call the NER API to classify columns\n",
    "            ner_response = classify_columns_with_api(df, type=\"fast\")\n",
    "\n",
    "            # Onboard the data into MongoDB\n",
    "            onboard_data(dataset, table_name, df, ne_cols, correct_qids, ner_response)\n",
    "\n",
    "    # Initialize dataset-level trace after onboarding all tables in the dataset\n",
    "    dataset_trace_collection.update_one(\n",
    "        {\"dataset_name\": dataset},\n",
    "        {\"$setOnInsert\": {\n",
    "            \"total_tables\": len(tables),\n",
    "            \"processed_tables\": 0,\n",
    "            \"total_rows\": 0,  # Updated as tables are processed\n",
    "            \"processed_rows\": 0,\n",
    "            \"status\": \"PENDING\"\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables...: 100%|██████████| 64/64 [01:28<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3\", \"2T_2020\", \"Round4_2020\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "# Function to get NE columns and correct QIDs from the GT file\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    # Initialize ne_cols as a dict where the value (NER type) is None initially\n",
    "    ne_cols = {str(col_index): None for col_index in filtered_cea_gt[2].unique()}\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{row[1]}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "# Function to call the NER API endpoint with multiple tables\n",
    "def classify_columns_with_api(tables_data, type=\"fast\"):\n",
    "    payload = {\n",
    "        \"json\": [df.T.astype(str).values.tolist() for df in tables_data]  # Multiple tables as input\n",
    "    }\n",
    "    url = f'https://lamapi.hel.sintef.cloud/sti/column-analysis?model_type={type}&token=lamapi_demo_2023'\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return JSON response with NER classifications\n",
    "    else:\n",
    "        print(f\"Error with API call: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main processing loop\n",
    "def process_tables(datasets, max_tables_at_once=1):\n",
    "    for dataset in datasets:\n",
    "        cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "        tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "        tables_data = []\n",
    "        processed_count = 0\n",
    "\n",
    "        for table in tqdm(tables, desc=\"Processing tables...\"):\n",
    "            if table.endswith(\".csv\"):\n",
    "                df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "                table_name = table.split(\".csv\")[0]\n",
    "\n",
    "                # Get NE columns and correct QIDs from GT\n",
    "                ne_cols, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "                \n",
    "                # Add table data to list\n",
    "                tables_data.append(df)\n",
    "                processed_count += 1\n",
    "\n",
    "                # If we reach the maximum number of tables, make the API call\n",
    "                if processed_count >= max_tables_at_once:\n",
    "                    # Call the NER API to classify columns for these tables\n",
    "                    ner_response = classify_columns_with_api(tables_data)\n",
    "                    \n",
    "                    if ner_response:\n",
    "                        for table_response in ner_response:\n",
    "                            table_key = list(table_response.keys())[0]\n",
    "                            lit_cols = {}  # Initialize LIT columns dictionary\n",
    "                            for col_idx, ner_info in table_response[table_key].items():\n",
    "                                col_num = str(ner_info[\"index_column\"])  # Get column index as string\n",
    "                                classification = ner_info[\"classification\"]  # Get NER or LIT type\n",
    "                                tag = ner_info[\"tag\"]\n",
    "\n",
    "                                # Update NE columns from GT\n",
    "                                if col_num in ne_cols:\n",
    "                                    # If GT classifies as NE, we trust the GT and update with the NER type\n",
    "                                    if tag == \"LIT\":\n",
    "                                        classification = \"OTHER\"\n",
    "                                    ne_cols[col_num] = classification\n",
    "                                elif tag == \"NE\":\n",
    "                                    # If API classifies as NE and not in GT, add to ne_cols\n",
    "                                    ne_cols[col_num] = classification\n",
    "                                elif tag == \"LIT\":\n",
    "                                    # If API classifies as LIT, add to lit_cols\n",
    "                                    lit_cols[col_num] = classification\n",
    "\n",
    "                    # Reset tables data for the next batch\n",
    "                    tables_data = []\n",
    "                    processed_count = 0\n",
    "        \n",
    "        # In case there are remaining tables that haven't been processed yet\n",
    "        if tables_data:\n",
    "            ner_response = classify_columns_with_api(tables_data)\n",
    "        return\n",
    "\n",
    "# Example of running the function\n",
    "process_tables(datasets, max_tables_at_once=10)  # Adjust the number of tables you want to submit at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables...: 100%|██████████| 64/64 [00:07<00:00,  8.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = [\"Round1_T2D\", \"Round3\", \"2T_2020\", \"Round4_2020\", \"HardTablesR2\", \"HardTablesR3\"]\n",
    "\n",
    "# Function to get NE columns and correct QIDs from the GT file\n",
    "def get_ne_cols_and_correct_qids(table_name, cea_gt):\n",
    "    correct_qids = {}\n",
    "    filtered_cea_gt = cea_gt[cea_gt[0] == table_name]\n",
    "    # Initialize ne_cols as a dict where the value (NER type) is None initially\n",
    "    ne_cols = {str(col_index): None for col_index in filtered_cea_gt[2].unique()}\n",
    "    for _, row in filtered_cea_gt.iterrows():\n",
    "        qid = row[3].split(\"/\")[-1]\n",
    "        correct_qids[f\"{row[1]}-{row[2]}\"] = qid\n",
    "    return ne_cols, correct_qids\n",
    "\n",
    "# Function to call the NER API endpoint with multiple tables\n",
    "def classify_columns_with_api(tables_data, type=\"fast\"):\n",
    "    payload = {\n",
    "        \"json\": [df.T.astype(str).values.tolist() for df in tables_data]  # Multiple tables as input\n",
    "    }\n",
    "    url = f'https://lamapi.hel.sintef.cloud/sti/column-analysis?model_type={type}&token=lamapi_demo_2023'\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return JSON response with NER classifications\n",
    "    else:\n",
    "        print(f\"Error with API call: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Main processing loop\n",
    "def process_tables(datasets, max_tables_at_once=1):\n",
    "    for dataset in datasets:\n",
    "        cea_gt = pd.read_csv(f\"./Datasets/{dataset}/gt/cea.csv\", header=None)\n",
    "        tables = os.listdir(f\"./Datasets/{dataset}/tables\")\n",
    "        tables_data = []\n",
    "        processed_count = 0\n",
    "\n",
    "        for table in tqdm(tables, desc=\"Processing tables...\"):\n",
    "            if table.endswith(\".csv\"):\n",
    "                df = pd.read_csv(f\"./Datasets/{dataset}/tables/{table}\")\n",
    "                table_name = table.split(\".csv\")[0]\n",
    "\n",
    "                # Get NE columns and correct QIDs from GT\n",
    "                ne_cols, correct_qids = get_ne_cols_and_correct_qids(table_name, cea_gt)\n",
    "                \n",
    "                # Add table data to list\n",
    "                tables_data.append(df)\n",
    "                processed_count += 1\n",
    "\n",
    "                # If we reach the maximum number of tables, make the API call\n",
    "                if processed_count >= max_tables_at_once:\n",
    "                    # Call the NER API to classify columns for these tables\n",
    "                    ner_response = classify_columns_with_api(tables_data)\n",
    "                    \n",
    "                    if ner_response:\n",
    "                        for table_response in ner_response:\n",
    "                            table_key = list(table_response.keys())[0]\n",
    "                            lit_cols = {}  # Initialize LIT columns dictionary\n",
    "                            for col_idx, ner_info in table_response[table_key].items():\n",
    "                                col_num = str(ner_info[\"index_column\"])  # Get column index as string\n",
    "                                classification = ner_info[\"classification\"]  # Get NER or LIT type\n",
    "                                tag = ner_info[\"tag\"]\n",
    "\n",
    "                                # Update NE columns from GT\n",
    "                                if col_num in ne_cols:\n",
    "                                    # If GT classifies as NE, we trust the GT and update with the NER type\n",
    "                                    if tag == \"LIT\":\n",
    "                                        classification = \"OTHER\"\n",
    "                                    ne_cols[col_num] = classification\n",
    "                                elif tag == \"NE\":\n",
    "                                    # If API classifies as NE and not in GT, add to ne_cols\n",
    "                                    ne_cols[col_num] = classification\n",
    "                                elif tag == \"LIT\":\n",
    "                                    # If API classifies as LIT, add to lit_cols\n",
    "                                    lit_cols[col_num] = classification\n",
    "\n",
    "                    # Reset tables data for the next batch\n",
    "                    tables_data = []\n",
    "                    processed_count = 0\n",
    "        \n",
    "        # In case there are remaining tables that haven't been processed yet\n",
    "        if tables_data:\n",
    "            ner_response = classify_columns_with_api(tables_data)\n",
    "        return\n",
    "\n",
    "# Example of running the function\n",
    "process_tables(datasets, max_tables_at_once=10)  # Adjust the number of tables you want to submit at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
